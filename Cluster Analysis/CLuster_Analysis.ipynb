{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Your_file_name.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Response-2020'].dropna().to_csv('cleaned_response-2020.csv', index = False)\n",
    "df_2020 = pd.read_csv('cleaned_response-2020.csv')\n",
    "print(df_2020.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Response-2021'].dropna().to_csv('cleaned_response-2021.csv', index = False)\n",
    "df_2021 = pd.read_csv('cleaned_response-2021.csv')\n",
    "print(df_2021.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_numbers(list_text):\n",
    "    list_text_new = []\n",
    "    for i in list_text:\n",
    "        if not re.search('\\d', i):\n",
    "            list_text_new.append(i)\n",
    "    return ''.join(list_text_new)\n",
    "\n",
    "df_2020['cleaned_data'] = df_2020['Response-2020'].apply(drop_numbers)\n",
    "df_2021['cleaned_data'] = df_2021['Response-2021'].apply(drop_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(text):\n",
    "    text_words = word_tokenize(text)\n",
    "    text_words_lower = [x.lower() for x in text_words]\n",
    "    return ' '.join(text_words_lower)\n",
    "\n",
    "df_2020['cleaned_data'] = df_2020['cleaned_data'].apply(lower_case)\n",
    "df_2021['cleaned_data'] = df_2021['cleaned_data'].apply(lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag[0].upper(), wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatise(text):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(text_tokens)\n",
    "    text_lemm = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
    "    return ' '.join(text_lemm)\n",
    "\n",
    "df_2020['cleaned_data'] = df_2020['cleaned_data'].apply(lemmatise)\n",
    "df_2021['cleaned_data'] = df_2021['cleaned_data'].apply(lemmatise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(text):\n",
    "    additional_stopwords = {'inaudible', \"n't\", 'u', 'oh', 's', 'r', 'uh', 'd', 'thing', 'think', 'still', 'much', 'anything', 'whatever','ve', 'sometimes', 'something'}\n",
    "    default_stopwords = set(stopwords.words('english'))\n",
    "    custom_stopwords = default_stopwords.union(additional_stopwords)\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens = [word for word in text_tokens if not word in custom_stopwords]\n",
    "    tokens_text = ' '.join(tokens)\n",
    "    return tokens_text\n",
    "\n",
    "df_2020['cleaned_data'] = df_2020['cleaned_data'].apply(remove_stopword)\n",
    "df_2021['cleaned_data'] = df_2021['cleaned_data'].apply(remove_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove filler words like 'umm' from text\n",
    "def remove_umm(text):\n",
    "    pattern = r'\\bum{1,}\\b' # Regular expression pattern for 'umm'\n",
    "    return re.sub(pattern, '', text) # Replace 'umm' with an empty string\n",
    "\n",
    "# Applying the remove_umm function to the cleaned_data column of 2020 and 2021 dataframes\n",
    "df_2020['cleaned_data'] = df_2020['cleaned_data'].apply(remove_umm)\n",
    "df_2021['cleaned_data'] = df_2021['cleaned_data'].apply(remove_umm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the modified dataframes to CSV files\n",
    "df_2020.to_csv('cleaned_response-2020.csv')\n",
    "df_2021.to_csv('cleaned_response-2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the cleaned_data columns from both dataframes into one series\n",
    "col1 = df_2020['cleaned_data']\n",
    "col2 = df_2021['cleaned_data']\n",
    "combined_column = pd.concat([col1, col2])\n",
    "combined_df = pd.DataFrame(combined_column) # Creating a dataframe from the combined series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the combined data using TF-IDF\n",
    "tfidf_matrix = vectorizer.fit_transform(combined_df['cleaned_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the feature names (words) from the vectorizer\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the TF-IDF matrix\n",
    "normalized_tfidf = normalize(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 1 # Setting the number of clusters for KMeans\n",
    "\n",
    "# Initializing and applying KMeans clustering\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(normalized_tfidf)\n",
    "\n",
    "# Adding the cluster labels to the dataframe\n",
    "combined_df['cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating word clouds for each cluster\n",
    "for i in range(num_clusters):\n",
    "    # Aggregate text for each cluster\n",
    "    cluster_text = ' '.join(combined_df[combined_df['cluster'] == i]['cleaned_data'])\n",
    "\n",
    "    # Generate a word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(cluster_text)\n",
    "\n",
    "    # Plot the word cloud\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Word Cloud for Cluster {i}')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
