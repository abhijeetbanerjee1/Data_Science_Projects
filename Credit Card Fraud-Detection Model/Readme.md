The project's main focus is to try and improve the previous fraud detection models by comparing various sampling techniques, classifiers, and performance metrics. We used a dataset generated by the transactions made by European cardholders in September 2013. All the transactions were made in two days, so the dataset is highly skewed. To solve this issue, we used various sampling methods on the dataset since machine learning models would learn better from a balanced dataset. Here we also see why accuracy is not a good metric for our problem and explore other metrics widely used for dealing with unbalanced data such as precision, recall & AUC-PR.

The dataset used for this research was obtained from Kaggle and comprises 284,807 credit card transactions made by European cardholders over 2 days in September 2013. It is extremely unbalanced because there are just 492 fraud incidents (positive class), which represent only 0.172% of all transactions. Due to security and privacy concerns, all original features and context were concealed. All features are numerical input variables where V1, V2, ..., and V28 were produced by applying PCA transformations to the original features. ‘Amount’ represents the transaction amount. "Time" was removed because it had no purpose in this study. The target variable ‘Class’ is 1 for fraudulent transactions and 0 for legal ones. Since the "Amount" feature has a varied range, it was scaled using StandardScalar(). Using train_test_split() and a test size of 20%, the dataset was divided into training and testing sets.

For this research, we carried out experiments using ensemble learning models like bagging, boosting, voting, and artificial neural networks along with oversampling (SMOTE) and a combination of oversampling and undersampling (ROS+RUS) to learn the dataset and identify the best learning model among them. More specifically, we used:
* Random Forest Classifier with 100 estimators (Decision trees) as an instance of Bagging that uses bootstrap samples of the training dataset.
*  XG Boost Classifier with a maximum depth of 3 for each tree as an instance of Boosting.
* Combination of base models such as Logistic Regression (with L2 penalty), Decision Tree Classifier, Gaussian Naïve Bayes, and K Nearest Neighbors with K=5 neighbors as an
instance of Voting Classifier.
* Multi-Layer Perceptron (MLP) as an instance of ANN that consists of two hidden layers
of size (100, 50), which uses Cross Entropy Loss and ReLu activation functions.

Through this study, we were able to explore different Sampling methods for handling unbalanced datasets, use Ensemble modeling for reducing generalization error of classification models, and understand how evaluation metrics like Precision, Recall, and AUC of recall vs precision, tell us how a model performs on an unbalanced distribution.
Some situations in the actual world cannot be avoided for a variety of reasons, much like class inequality. There are still additional machine learning techniques that may be used to provide predictions that are more accurate and realistic, such as cost-sensitive learning, various learning algorithms, anomaly detection, and the use of measures like false positives, true positives, F1-score, etc. Imposing an effective approach for significantly identifying fraudulent transactions without blaming customers is critical to resolving this slowly prevalent global challenge.
